# Moonboard Grade Prediction - Technical Specification

## 1. Problem Statement and Objectives

### 1.1 Overview

The Moonboard Grade Prediction system is a machine learning solution designed to predict the difficulty grade of climbing problems on a Moonboard training wall. Given a configuration of holds (starting positions, intermediate holds, and ending positions), the system predicts the Font scale grade of the problem.

### 1.2 Objectives

**Primary Objective**: Build a classification neural network that accurately predicts Font scale grades (5+ through 8C+) from Moonboard hold positions.

**Secondary Objectives**:
- Achieve high accuracy within Â±1 grade tolerance (more useful than exact accuracy for climbing applications)
- Provide probability distributions over all possible grades for confidence estimation
- Support both batch and single-problem prediction workflows
- Enable easy model training, evaluation, and deployment

### 1.3 Use Cases

1. **Problem Setters**: Validate the difficulty of new problems before publishing
2. **Climbers**: Get difficulty estimates for custom problem configurations
3. **Training Apps**: Automatically suggest problems of appropriate difficulty
4. **Data Analysis**: Understand what hold configurations correlate with different grades

---

## 2. Data Representation

### 2.1 Input: Moonboard Grid

The Moonboard is represented as an **11Ã—18 grid**:
- **Columns**: A through K (11 columns, indexed 0-10)
- **Rows**: 1 through 18 (18 rows, indexed 0-17 from bottom to top)
- **Total positions**: 198 possible hold locations

Each position is identified by a string like `"F7"` (column F, row 7).

### 2.2 Tensor Encoding

Problems are encoded as **3-channel tensors** with shape `(3, 18, 11)`:

```
Channel 0 (Start Holds):    Binary mask where holds have isStart=True
Channel 1 (Middle Holds):   Binary mask where holds have isStart=False and isEnd=False  
Channel 2 (End Holds):      Binary mask where holds have isEnd=True
```

**Properties**:
- Data type: `float32` (PyTorch compatibility)
- Values: Binary (0.0 or 1.0)
- Spatial dimensions: Height=18, Width=11 (consistent with image convention)

**Example**: A problem with start holds at A1 and B1, middle holds at C5 and D8, and end hold at F18:
```
tensor[0, 0, 0] = 1.0  # A1 is start
tensor[0, 0, 1] = 1.0  # B1 is start
tensor[1, 4, 2] = 1.0  # C5 is middle
tensor[1, 7, 3] = 1.0  # D8 is middle
tensor[2, 17, 5] = 1.0 # F18 is end
```

### 2.3 Output: Grade Labels

**Font Scale Grades** (19 classes, indexed 0-18):

| Index | Grade | Index | Grade | Index | Grade | Index | Grade |
|-------|-------|-------|-------|-------|-------|-------|-------|
| 0     | 5+    | 5     | 6C    | 10    | 7B    | 15    | 8A+   |
| 1     | 6A    | 6     | 6C+   | 11    | 7B+   | 16    | 8B    |
| 2     | 6A+   | 7     | 7A    | 12    | 7C    | 17    | 8B+   |
| 3     | 6B    | 8     | 7A+   | 13    | 7C+   | 18    | 8C+   |
| 4     | 6B+   | 9     | 7B    | 14    | 8A    |       |       |

**Encoding**:
- Grade strings are case-insensitive and whitespace-tolerant
- Invalid grades raise `ValueError`
- Bidirectional conversion supported (string â†” integer)

---

## 3. Model Architectures

Two architectures are implemented for comparison: a Fully Connected baseline and a Convolutional network.

### 3.1 Fully Connected Model (FC)

**Architecture**:
```
Input: (batch, 3, 18, 11)
  â†“
Flatten â†’ (batch, 594)
  â†“
Linear(594 â†’ 256)
  â†“
ReLU
  â†“
Dropout(p=0.3)
  â†“
Linear(256 â†’ 128)
  â†“
ReLU
  â†“
Dropout(p=0.3)
  â†“
Linear(128 â†’ num_classes)
  â†“
Output: (batch, num_classes) logits
```

**Parameters** (for 19 classes):
- Layer 1: 594 Ã— 256 + 256 = 152,064 + 256 = 152,320
- Layer 2: 256 Ã— 128 + 128 = 32,768 + 128 = 32,896
- Layer 3: 128 Ã— 19 + 19 = 2,432 + 19 = 2,451
- **Total: 187,667 parameters**

**Characteristics**:
- Simple baseline architecture
- Treats all holds equally (no spatial awareness)
- Fast training and inference
- Works well for problems where absolute positions matter more than spatial relationships

### 3.2 Convolutional Model (CNN)

**Architecture**:
```
Input: (batch, 3, 18, 11)
  â†“
Conv2d(3 â†’ 16, kernel=3Ã—3, padding=1)
  â†“
ReLU
  â†“
MaxPool2d(2Ã—2) â†’ (batch, 16, 9, 5)
  â†“
Conv2d(16 â†’ 32, kernel=3Ã—3, padding=1)
  â†“
ReLU
  â†“
MaxPool2d(2Ã—2) â†’ (batch, 32, 4, 2)
  â†“
Flatten â†’ (batch, 256)
  â†“
Linear(256 â†’ 128)
  â†“
ReLU
  â†“
Dropout(p=0.5)
  â†“
Linear(128 â†’ num_classes)
  â†“
Output: (batch, num_classes) logits
```

**Spatial Dimension Changes**:
1. Input: 18 Ã— 11
2. After Conv1 + Pool1: âŒŠ18/2âŒ‹ Ã— âŒŠ11/2âŒ‹ = 9 Ã— 5
3. After Conv2 + Pool2: âŒŠ9/2âŒ‹ Ã— âŒŠ5/2âŒ‹ = 4 Ã— 2
4. Flattened: 32 Ã— 4 Ã— 2 = 256

**Parameters** (for 19 classes):
- Conv1: (3 Ã— 3 Ã— 3 + 1) Ã— 16 = 28 Ã— 16 = 448
- Conv2: (3 Ã— 3 Ã— 16 + 1) Ã— 32 = 145 Ã— 32 = 4,640
- FC1: 256 Ã— 128 + 128 = 32,768 + 128 = 32,896
- FC2: 128 Ã— 19 + 19 = 2,432 + 19 = 2,451
- **Total: 40,435 parameters**

**Characteristics**:
- Learns spatial relationships between holds
- Captures local patterns (e.g., holds close together, diagonal sequences)
- More parameters but still relatively lightweight
- Better suited for problems where hold proximity and patterns matter

### 3.3 Model Selection

Use the `create_model(model_type, num_classes)` factory function:
```python
from src.models import create_model

# Create FC model
fc_model = create_model('fc', num_classes=19)

# Create CNN model
cnn_model = create_model('cnn', num_classes=19)
```

---

## 4. Training Methodology

### 4.1 Loss Function

**Cross-Entropy Loss** (`nn.CrossEntropyLoss`):
- Standard for multi-class classification
- Combines LogSoftmax and NLLLoss
- Penalizes incorrect predictions exponentially
- Formula: `L = -log(exp(x[class]) / Î£ exp(x[i]))`

### 4.2 Optimizer

**Adam Optimizer** (default):
- Adaptive learning rate per parameter
- Default learning rate: 0.001
- Betas: (0.9, 0.999)
- Epsilon: 1e-8

**Alternative: SGD**:
- Learning rate: 0.01
- Momentum: 0.9
- Useful for comparison and potentially better generalization

### 4.3 Training Hyperparameters

**Default Configuration** (from `config.yaml`):
```yaml
training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
  optimizer: adam
  early_stopping_patience: 10
```

**Data Splits** (stratified):
- Training: 70%
- Validation: 15%
- Test: 15%

Stratification ensures each split maintains the same grade distribution as the full dataset, critical for imbalanced climbing grade data.

### 4.4 Early Stopping

**Mechanism**:
- Monitor validation loss after each epoch
- Save checkpoint when validation loss improves
- Track epochs without improvement
- Stop training if no improvement for `patience` epochs

**Benefits**:
- Prevents overfitting
- Reduces unnecessary computation
- Automatically finds optimal training duration

**Implementation**:
```python
if val_loss < best_val_loss:
    best_val_loss = val_loss
    epochs_without_improvement = 0
    save_checkpoint()  # Save best model
else:
    epochs_without_improvement += 1
    if epochs_without_improvement >= patience:
        break  # Stop training
```

### 4.5 Checkpointing

**Checkpoint Contents**:
```python
{
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': current_epoch,
    'history': training_history,
    'best_val_loss': best_val_loss
}
```

**Checkpoint Types**:
1. **Best Checkpoint** (`best_model.pth`): Saved when validation loss improves
2. **Final Checkpoint** (`final_model.pth`): Saved at end of training

### 4.6 Training Metrics

**Tracked per Epoch**:
- Training loss (average over all batches)
- Validation loss
- Validation accuracy (exact match percentage)

**History Structure**:
```python
{
    'train_loss': [epoch1_loss, epoch2_loss, ...],
    'val_loss': [epoch1_loss, epoch2_loss, ...],
    'val_accuracy': [epoch1_acc, epoch2_acc, ...]
}
```

---

## 5. Evaluation Metrics

### 5.1 Accuracy Metrics

#### 5.1.1 Exact Accuracy

Percentage of predictions that exactly match the true grade.

**Formula**: `exact_accuracy = (correct_predictions / total_predictions) Ã— 100%`

**Interpretation**: Strict metric; useful for understanding perfect prediction rate.

#### 5.1.2 Tolerance Accuracy

Percentage of predictions within Â±N grades of the true grade.

**Common Tolerances**:
- **Â±1 grade**: Most useful for climbing applications (e.g., predicting 6B for a 6B+ is acceptable)
- **Â±2 grades**: Very lenient; measures if model understands general difficulty tier

**Formula**: 
```
tolerance_accuracy(N) = (predictions where |predicted - actual| â‰¤ N) / total Ã— 100%
```

**Example**:
- True grade: 7A (index 7)
- Predicted: 6C+ (index 6)
- Difference: |6 - 7| = 1
- Result: Exact accuracy = 0%, Â±1 accuracy = 100%, Â±2 accuracy = 100%

#### 5.1.3 Mean Absolute Error (MAE)

Average number of grades off from the true grade.

**Formula**: `MAE = (Î£ |predicted - actual|) / total`

**Interpretation**: Lower is better; 0 = perfect, 1 = average of Â±1 grade off

### 5.2 Per-Grade Metrics

Calculated using scikit-learn's `precision_recall_fscore_support`:

**Precision**: Of all predictions for grade X, what percentage were correct?
- Formula: `precision = true_positives / (true_positives + false_positives)`
- High precision = few false alarms for that grade

**Recall**: Of all problems with grade X, what percentage were correctly identified?
- Formula: `recall = true_positives / (true_positives + false_negatives)`
- High recall = few missed instances of that grade

**F1-Score**: Harmonic mean of precision and recall
- Formula: `F1 = 2 Ã— (precision Ã— recall) / (precision + recall)`
- Balanced metric when you care about both precision and recall

**Support**: Number of actual instances of that grade in the dataset

### 5.3 Confusion Matrix

**Structure**: `num_classes Ã— num_classes` matrix where:
- Rows: Actual grades
- Columns: Predicted grades
- Cell (i, j): Number of times grade i was predicted as grade j

**Diagonal Elements**: Correct predictions (should be high)

**Off-Diagonal Elements**: Misclassifications
- Elements near diagonal: "Close" errors (Â±1-2 grades)
- Elements far from diagonal: Serious errors

**Visualization**: Heatmap with:
- Normalized values (percentages) for better interpretation
- Color intensity shows frequency
- Grade labels on both axes
- Can be saved as PNG for reports

### 5.4 Comprehensive Evaluation

Use `get_metrics_summary()` for complete evaluation:

```python
from src.evaluator import get_metrics_summary

metrics = get_metrics_summary(
    model=trained_model,
    dataloader=test_loader,
    device='cpu',
    save_cm_path='confusion_matrix.png'
)
```

**Returns**:
```python
{
    'exact_accuracy': 45.2,
    'tolerance_1_accuracy': 78.5,
    'tolerance_2_accuracy': 92.1,
    'mean_absolute_error': 0.68,
    'per_grade_metrics': {
        '6A': {'precision': 0.52, 'recall': 0.48, 'f1': 0.50, 'support': 42},
        '6B': {'precision': 0.61, 'recall': 0.58, 'f1': 0.59, 'support': 65},
        # ... for all grades
    },
    'confusion_matrix': [[...], [...], ...]  # numpy array
}
```

---

## 6. API Specification

### 6.1 Predictor Interface

The `Predictor` class provides the inference API.

#### 6.1.1 Initialization

```python
from src.predictor import Predictor

predictor = Predictor(
    checkpoint_path='models/best_model.pth',
    device='cpu'  # or 'cuda'
)
```

**Parameters**:
- `checkpoint_path`: Path to saved model checkpoint (.pth file)
- `device`: 'cpu' or 'cuda' (automatically validated)

**Automatic Features**:
- Infers model architecture (FC vs CNN) from checkpoint
- Sets model to evaluation mode
- Handles device placement

#### 6.1.2 Single Problem Prediction

```python
problem = {
    "moves": [
        {"Description": "A1", "isStart": True, "isEnd": False},
        {"Description": "C5", "isStart": False, "isEnd": False},
        {"Description": "F18", "isStart": False, "isEnd": True}
    ]
}

result = predictor.predict(problem, top_k=3)
```

**Input Format**:
```python
{
    "moves": [
        {
            "Description": str,  # Position like "F7"
            "isStart": bool,     # True if starting hold
            "isEnd": bool        # True if ending hold
        },
        # ... more moves
    ],
    "grade": str  # Optional, for validation
}
```

**Output Format**:
```python
{
    "predicted_grade": "6B+",           # Most likely grade
    "confidence": 0.62,                 # Probability of top prediction
    "probabilities": {                  # All grade probabilities
        "5+": 0.001,
        "6A": 0.015,
        # ... (all 19 grades)
        "8C+": 0.0003
    },
    "top_predictions": [                # Top K predictions
        {"grade": "6B+", "probability": 0.62},
        {"grade": "6C", "probability": 0.23},
        {"grade": "6B", "probability": 0.09}
    ]
}
```

#### 6.1.3 Batch Prediction

```python
problems = [problem1, problem2, problem3]
results = predictor.predict_batch(problems, top_k=3)
# Returns list of prediction dictionaries
```

**Error Handling**: Individual problems that fail return error information instead of predictions.

#### 6.1.4 Tensor Prediction

For pre-processed tensors:

```python
import numpy as np

tensor = np.zeros((3, 18, 11), dtype=np.float32)
# ... populate tensor ...

result = predictor.predict_from_tensor(tensor, top_k=3)
```

**Supports**:
- NumPy arrays and PyTorch tensors
- Single samples `(3, 18, 11)` or batches `(N, 3, 18, 11)`

#### 6.1.5 Model Information

```python
info = predictor.get_model_info()
```

**Returns**:
```python
{
    'model_type': 'Convolutional',  # or 'FullyConnected'
    'num_parameters': 40435,
    'num_classes': 19,
    'device': 'cpu',
    'checkpoint_path': 'models/best_model.pth'
}
```

### 6.2 Command-Line Interface

#### 6.2.1 Training

```bash
python main.py train --config config.yaml
```

**Configuration File** (`config.yaml`):
```yaml
model:
  type: cnn              # 'fc' or 'cnn'
  num_classes: 19

training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
  optimizer: adam        # 'adam' or 'sgd'
  early_stopping_patience: 10

data:
  train_path: data/problems.json
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  random_seed: 42

checkpoints:
  directory: models
  save_best: true
  save_final: true

device:
  type: cuda            # 'cuda' or 'cpu' (auto-fallback)
```

**Outputs**:
- Best model checkpoint: `models/best_model.pth`
- Final model checkpoint: `models/final_model.pth`
- Training history: `models/training_history.json`
- Confusion matrix: `models/confusion_matrix.png`

#### 6.2.2 Evaluation

```bash
python main.py evaluate \
    --checkpoint models/best_model.pth \
    --data data/test_problems.json \
    --confusion-matrix results/cm.png
```

**Console Output**:
```
Model Evaluation Results:
Exact Accuracy: 45.2%
Â±1 Grade Accuracy: 78.5%
Â±2 Grade Accuracy: 92.1%
Mean Absolute Error: 0.68 grades

Per-Grade Metrics:
Grade   Precision  Recall    F1-Score  Support
6A      52.3%      48.1%     50.1%     42
6B      61.2%      58.4%     59.8%     65
...
```

#### 6.2.3 Prediction

**Single Problem**:
```bash
python main.py predict \
    --checkpoint models/best_model.pth \
    --input problem.json \
    --top-k 3 \
    --output predictions.json
```

**Input File** (`problem.json`):
```json
{
    "moves": [
        {"Description": "A1", "isStart": true, "isEnd": false},
        {"Description": "C5", "isStart": false, "isEnd": false},
        {"Description": "F18", "isStart": false, "isEnd": true}
    ],
    "grade": "6B+"
}
```

**Console Output**:
```
ðŸŽ¯ Prediction Results

Top 3 Predictions:
  1. 6B+ (62.3%)
  2. 6C  (23.1%)
  3. 6B  (9.2%)

Actual Grade: 6B+
âœ“ Prediction matches actual grade!
```

**Batch Mode**:
```bash
python main.py predict \
    --checkpoint models/best_model.pth \
    --input problems.json \  # File with multiple problems
    --output batch_predictions.json
```

---

## 7. Performance Benchmarks

### 7.1 Model Comparison

**Expected Performance** (dataset-dependent):

| Metric                  | FC Model | CNN Model |
|------------------------|----------|-----------|
| Exact Accuracy         | 40-50%   | 45-55%    |
| Â±1 Grade Accuracy      | 75-85%   | 80-90%    |
| Â±2 Grade Accuracy      | 90-95%   | 92-97%    |
| Mean Absolute Error    | 0.7-0.9  | 0.5-0.7   |
| Training Time (100 ep) | ~5 min   | ~8 min    |
| Inference (1 problem)  | <1 ms    | <2 ms     |
| Parameters             | 187,667  | 40,435    |

*Note: Performance varies significantly based on dataset size, quality, and grade distribution*

### 7.2 Training Characteristics

**Typical Training Curves**:
- Loss decreases rapidly in first 10-20 epochs
- Validation accuracy plateaus around epoch 30-50
- Early stopping typically triggers between epochs 40-80

**Common Issues**:
- **Overfitting**: Validation loss increases while training loss decreases
  - Solution: Increase dropout, reduce model capacity, or get more data
- **Underfitting**: Both losses high and not decreasing
  - Solution: Increase model capacity, train longer, reduce dropout
- **Class Imbalance**: Poor performance on rare grades
  - Mitigation: Stratified splitting helps, but more data for rare grades is ideal

### 7.3 Computational Requirements

**Minimum Requirements**:
- RAM: 4 GB
- Storage: 100 MB (including models and data)
- CPU: Any modern processor (2+ cores recommended)

**Recommended for Training**:
- RAM: 8 GB+
- GPU: NVIDIA GPU with CUDA support (optional but faster)
- Storage: 500 MB+ for large datasets

**Inference Requirements**:
- Very lightweight: Can run on any device
- CPU inference: <2ms per problem
- GPU inference: <0.5ms per problem

---

## 8. Known Limitations and Future Improvements

### 8.1 Current Limitations

#### 8.1.1 Data Requirements

**Limitation**: Model quality depends heavily on dataset size and diversity
- Rare grades (8A+, 8B, etc.) often have insufficient training examples
- Grade distribution is typically imbalanced (more intermediate problems)

**Impact**: Poor predictions for underrepresented grades

#### 8.1.2 Positional Information Only

**Limitation**: Only uses hold positions, ignoring other important factors:
- Hold type (jug, crimp, sloper, pinch)
- Wall angle (vertical, overhang degree)
- Climber attributes (height, ape index, strength)

**Impact**: Two problems with same holds but different hold types can have very different grades

#### 8.1.3 Spatial Context

**Limitation**: Even CNN has limited receptive field
- May not capture full-body positioning requirements
- Doesn't understand climbing sequences explicitly

**Impact**: May miss aspects like "this problem requires a big dyno" or "this requires extreme flexibility"

#### 8.1.4 Grading Subjectivity

**Limitation**: Climbing grades are subjective and vary by:
- Setter interpretation
- Community consensus over time
- Regional differences
- Climber body types and styles

**Impact**: Model can't be "more accurate" than the subjective ground truth

### 8.2 Future Improvements

#### 8.2.1 Enhanced Input Features

**Addition of Hold Types**:
- Expand from 3 channels to 12+ (3 hold positions Ã— 4 hold types)
- Or use categorical embedding for hold types
- Expected improvement: 5-10% accuracy increase

**Wall Angle Information**:
- Add scalar input or additional channel for wall angle
- Critical for difficulty assessment (vertical vs 45Â° overhang)

**Sequential Information**:
- Encode the climbing sequence (order of moves)
- Use RNN/LSTM or Transformer architecture
- Could capture dynamic movement requirements

#### 8.2.2 Advanced Architectures

**Graph Neural Networks (GNN)**:
- Represent problem as graph: nodes = holds, edges = possible transitions
- Captures relationships between holds naturally
- Better understands sequence and reach requirements

**Attention Mechanisms**:
- Learn which holds are most important for difficulty
- Provide interpretable predictions (highlight key moves)

**Multi-Task Learning**:
- Simultaneously predict grade and other attributes (style tags, quality rating)
- Share representations, potentially improve all tasks

#### 8.2.3 Data Augmentation

**Horizontal Flipping**:
- Mirror problems left-right (A â†” K)
- Doubles dataset size
- Most problems are grade-equivalent when mirrored

**Grade Smoothing**:
- Use soft labels (probability distribution around true grade)
- Account for grading uncertainty
- May improve generalization

**Synthetic Problems**:
- Generate new problems by interpolating between similar-grade problems
- Carefully validate quality

#### 8.2.4 Uncertainty Quantification

**Bayesian Neural Networks**:
- Provide confidence intervals, not just point predictions
- Important for safety-critical applications

**Ensemble Methods**:
- Train multiple models, combine predictions
- More robust predictions, better uncertainty estimates

**Conformal Prediction**:
- Provide prediction sets with guaranteed coverage
- "90% confident the grade is between 6B and 6C+"

#### 8.2.5 Active Learning

**Selective Labeling**:
- Identify problems where model is most uncertain
- Prioritize getting more training data for those configurations
- Maximize learning per labeled example

#### 8.2.6 Explainability

**Gradient-Based Visualization**:
- Highlight which holds contribute most to difficulty prediction
- Help setters understand what makes problem hard

**Counterfactual Explanations**:
- "If you moved hold F7 to F8, grade would likely increase from 6B to 6B+"
- Actionable feedback for problem design

#### 8.2.7 Deployment Enhancements

**Web API**:
- RESTful API for predictions
- Serve model using Flask/FastAPI
- Enable integration with web applications

**Mobile Optimization**:
- Convert to ONNX or TensorFlow Lite
- Run inference on mobile devices
- Useful for gym applications

**Real-Time Feedback**:
- As setter places holds, show live grade estimate
- Interactive problem design tool

### 8.3 Research Directions

#### 8.3.1 Transfer Learning

- Pre-train on large database of general climbing problems
- Fine-tune on specific Moonboard setups (2016, 2017, 2019, Masters)
- Investigate if models generalize across different board types

#### 8.3.2 Style Classification

- Multi-label classification: predict problem style tags
  - "dynos", "crimpy", "slopey", "technical", "powerful"
- Helps climbers find problems matching their strengths/weaknesses

#### 8.3.3 Personalized Difficulty

- Account for climber-specific attributes
- "This problem is 6B+ for average climber, but 6A+ for you (tall with good finger strength)"
- Requires climber profile data and performance history

#### 8.3.4 Adversarial Testing

- Find minimal changes to problem that cause grade change
- Understand decision boundaries
- Identify which hold placements are most critical

---

## 9. Implementation Notes

### 9.1 Code Organization

**Modular Design**: Each component is independent and testable
- Easy to modify individual components
- Facilitates experimentation with different architectures

**Test Coverage**: 360+ tests covering all components
- Unit tests for each module
- Integration tests for full workflows
- Ensures reliability and catches regressions

**Documentation**: Comprehensive docstrings and type hints
- Clear API contracts
- Easy for new contributors to understand

### 9.2 Dependencies

**Core**:
- PyTorch 2.0+ (deep learning framework)
- NumPy 1.24+ (numerical operations)
- pandas 2.0+ (data manipulation, if needed)

**Supporting**:
- scikit-learn 1.3+ (train/test splitting, metrics)
- matplotlib 3.7+ (visualizations)
- seaborn 0.12+ (confusion matrix heatmaps)
- PyYAML 6.0+ (configuration files)

**Development**:
- pytest 7.4+ (testing framework)
- pytest-cov (test coverage)

### 9.3 Reproducibility

**Random Seed Control**:
```python
import random
import numpy as np
import torch

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
```

**Configuration Management**:
- All hyperparameters in `config.yaml`
- Version control configurations with code
- Track which config produced which results

**Data Versioning**:
- Hash datasets to track versions
- Store data provenance (source, date collected, processing steps)

### 9.4 Best Practices

**Training**:
1. Start with small dataset to verify pipeline works
2. Use FC model as baseline before trying CNN
3. Monitor both training and validation metrics
4. Save checkpoints frequently
5. Use early stopping to prevent overfitting

**Evaluation**:
1. Always use held-out test set (never used in training)
2. Report multiple metrics (exact, Â±1, Â±2, MAE)
3. Analyze confusion matrix to understand error patterns
4. Check per-grade metrics for imbalanced performance

**Deployment**:
1. Test predictions on known problems before deploying
2. Monitor prediction distributions (shouldn't be too concentrated)
3. Collect user feedback on predictions
4. Retrain periodically with new data

---

## 10. Conclusion

This specification documents a complete end-to-end system for Moonboard grade prediction using neural networks. The implementation prioritizes:

- **Simplicity**: Clear, modular code that's easy to understand and modify
- **Robustness**: Comprehensive testing and error handling
- **Flexibility**: Support for multiple architectures and easy experimentation
- **Practicality**: Focus on metrics that matter for real climbing applications

The system provides a solid foundation for climbing grade prediction while leaving room for numerous enhancements as outlined in Section 8. The modular design makes it straightforward to experiment with new ideas while maintaining a stable, tested core.

**Key Takeaway**: Predicting climbing grades from hold positions is a challenging problem due to data limitations and grading subjectivity, but neural networks can learn useful patterns that achieve 75-90% accuracy within Â±1 grade, making them valuable tools for problem setters and climbers.

