# Conditional VAE Training Configuration

# Data configuration
data:
  data_path: "../data/problems.json"
  train_split: 0.8  # 80% training, 20% validation
  batch_size: 64
  num_workers: 0  # Set to 0 for Windows compatibility
  min_grade_index: 2
  max_grade_index: 12

# Model architecture
model:
  latent_dim: 128
  grade_embedding_dim: 32

# Training hyperparameters
training:
  num_epochs: 70
  learning_rate: 0.0005
  max_grad_norm: 1.0  # Clip global gradient norm before optimizer step
  kl_weight: 0.01  # Reduced from 1.0 to prevent KL from dominating
  kl_annealing: true  # Gradually increase KL weight from 0.01 to kl_weight
  kl_annealing_epochs: 10  # Number of epochs to anneal over

# Checkpoint and logging
checkpoint:
  checkpoint_dir: "models"
  
logging:
  log_dir: "runs"
  log_interval: 100  # Log batch progress every N batches

# Device configuration
device: "cuda"  # Use "cuda" if available, otherwise "cpu"
