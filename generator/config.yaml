# Conditional VAE Training Configuration

# Data configuration
data:
  data_path: "../data/problems.json"
  train_split: 0.8  # 80% training, 20% validation
  batch_size: 64
  num_workers: 0  # Set to 0 for Windows compatibility
  min_grade_index: 2
  max_grade_index: 12

# Model architecture
model:
  latent_dim: 128
  grade_embedding_dim: 32
  dropout_rate: 0.1

# Training hyperparameters
training:
  num_epochs: 70
  learning_rate: 0.001
  weight_decay: 1.0e-5
  max_grad_norm: 1.0  # Clip global gradient norm before optimizer step
  kl_weight: 0.5  
  kl_annealing: true  # Gradually increase KL weight from 0.01 to kl_weight
  kl_annealing_epochs: 10  # Longer annealing to reduce early objective drift
  early_stopping_patience: 25  # More room before stopping while objective is still settling
  early_stopping_min_delta: 0.0005  # Avoid resetting patience on tiny/noisy fluctuations

# Checkpoint and logging
checkpoint:
  checkpoint_dir: "models"
  
logging:
  log_dir: "runs"
  log_interval: 100  # Log batch progress every N batches

# Device configuration
device: "cuda"  # Use "cuda" if available, otherwise "cpu"
